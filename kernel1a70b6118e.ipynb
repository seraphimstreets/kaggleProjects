{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as mtr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "from string import punctuation\n",
    "import re\n",
    "import tensorflow_probability as tfp\n",
    "from scipy.spatial import Voronoi\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from keras.layers import Dropout,  BatchNormalization,  Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n",
    "\n",
    "\n",
    "outcomes = train[['GameId','PlayId','Yards']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = ['OffenseFormation']\n",
    "of_index = {o:i for i,o in enumerate(train['OffenseFormation'].unique())}\n",
    "#op_index = {o:i for i,o in enumerate(train['OffensePersonnel'].unique())}\n",
    "#dp_index = {o:i for i,o in enumerate(train['DisplayName'].unique())}\n",
    "week_index = {o:i for i,o in enumerate(train['Quarter'].unique())}\n",
    "\n",
    "train['OffenseFormation'] = train['OffenseFormation'].apply(lambda x: of_index[x])\n",
    "#train['OffensePersonnel'] = train['OffensePersonnel'].apply(lambda x: op_index[x])\n",
    "#train['DisplayName'] = train['DisplayName'].apply(lambda x: dp_index[x])\n",
    "train['Quarter'] = train['Quarter'].apply(lambda x: week_index[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_orig(df, deploy=False):\n",
    "    def new_X(x_coordinate, play_direction):\n",
    "        if play_direction == 'left':\n",
    "            return 120.0 - x_coordinate\n",
    "        else:\n",
    "            return x_coordinate\n",
    "\n",
    "    def new_line(rush_team, field_position, yardline):\n",
    "        if rush_team == field_position:\n",
    "            # offense starting at X = 0 plus the 10 yard endzone plus the line of scrimmage\n",
    "            return 10.0 + yardline\n",
    "        else:\n",
    "            # half the field plus the yards between midfield and the line of scrimmage\n",
    "            return 60.0 + (50 - yardline)\n",
    "\n",
    "    def new_orientation(angle, play_direction):\n",
    "        if play_direction == 'left':\n",
    "            new_angle = 360.0 - angle\n",
    "            if new_angle == 360.0:\n",
    "                new_angle = 0.0\n",
    "            return new_angle\n",
    "        else:\n",
    "            return angle\n",
    "\n",
    "    def euclidean_distance(x1,y1,x2,y2):\n",
    "        x_diff = (x1-x2)**2\n",
    "        y_diff = (y1-y2)**2\n",
    "\n",
    "        return np.sqrt(x_diff + y_diff)\n",
    "\n",
    "    def back_direction(orientation):\n",
    "        if orientation > 180.0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def get_voronoi(play_id, df=df):\n",
    "        df = df[df.PlayId == play_id]\n",
    "        xy = df[['std_X', 'std_Y']].values\n",
    "        n_points = xy.shape[0]\n",
    "        xy1 = xy.copy()\n",
    "        xy1[:,1] = - xy[:,1]\n",
    "        xy2 = xy.copy()\n",
    "        xy2[:,1] = 320/3 - xy[:,1]\n",
    "        xy3 = xy.copy()\n",
    "        xy3[:,0] = 20 - xy[:,0]\n",
    "        xy4 = xy.copy()\n",
    "        xy4[:,0] = 220 - xy[:,0]\n",
    "        xy = np.concatenate((xy, xy1, xy2, xy3, xy4), axis=0)\n",
    "        offense = df.IsOnOffense.values\n",
    "\n",
    "        rusher_val = df.IsRusher.values\n",
    "        vor = Voronoi(xy)\n",
    "        offense_area = 0\n",
    "        defense_area = 0\n",
    "        rusher_area = 0\n",
    "        for r in range(n_points):\n",
    "            region = vor.regions[vor.point_region[r]]\n",
    "\n",
    "\n",
    "            if not -1 in region:\n",
    "                polygon = [vor.vertices[i] for i in region]\n",
    "                x_values, y_values = np.array([a[0] for a in polygon]),np.array([a[1] for a in polygon])\n",
    "                if offense[r]:\n",
    "                    offense_area += PolyArea(x_values, y_values)\n",
    "                    if rusher_val[r]:\n",
    "                        rusher_area = PolyArea(x_values, y_values)\n",
    "                else:\n",
    "                    defense_area += PolyArea(x_values, y_values)\n",
    "\n",
    "        return offense_area, defense_area, rusher_area\n",
    "\n",
    "    def PolyArea(x,y):\n",
    "        return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "    def update_yardline(df):\n",
    "        new_yardline = df[df['NflId'] == df['NflIdRusher']]\n",
    "        new_yardline['YardLine'] = new_yardline[['PossessionTeam','FieldPosition','YardLine']].apply(lambda x: new_line(x[0],x[1],x[2]), axis=1)\n",
    "        new_yardline = new_yardline[['GameId','PlayId','YardLine']]\n",
    "\n",
    "        return new_yardline\n",
    "\n",
    "    def update_orientation(df, yardline):\n",
    "     \n",
    "        df['Orientation'] = df[['Orientation','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n",
    "        df['Dir'] = df[['Dir','PlayDirection']].apply(lambda x: new_orientation(x[0],x[1]), axis=1)\n",
    "\n",
    "        df = df.drop('YardLine', axis=1)\n",
    "        df = pd.merge(df, yardline, on=['GameId','PlayId'], how='inner')\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def add_voronoi(df):\n",
    "        df['OffenseTeam'] = np.repeat(df.loc[(df['NflId'] == df['NflIdRusher']), 'Team'].values, 22)\n",
    "    \n",
    "        df['std_X'] = df.X\n",
    "        df.loc[df.PlayDirection == 'left', 'std_X'] = 120 - df.loc[train.PlayDirection == 'left', 'X'] \n",
    "        df['std_Y'] = df.Y\n",
    "        df.loc[df.PlayDirection == 'left', 'std_X'] = 160/3 - df.loc[train.PlayDirection == 'left', 'Y'] \n",
    "        df['IsRusher'] = (df['NflIdRusher'] == df['NflId'])\n",
    "        df['IsOnOffense'] = (df['Team'] == df['OffenseTeam'])\n",
    "\n",
    "\n",
    "        voronoi_stats = []    \n",
    "        for pid in df.PlayId.unique():\n",
    "            voi = get_voronoi(pid, df)\n",
    "            voronoi_stats.append(voi)\n",
    "            \n",
    "        df = pd.concat([df, pd.DataFrame(np.repeat(np.array(voronoi_stats), 22, axis=0), columns=['off_voi', 'def_voi', 'rusher_voi'])], axis=1)\n",
    "            \n",
    "        return df\n",
    " \n",
    "\n",
    "    def projection_features(df):\n",
    "        rad = 2 * np.pi * (90 - df[['Orientation']]) / 360\n",
    "        v_0 = df['S'].values * np.cos(rad).values.reshape(-1)\n",
    "        v_1 = np.sin(rad).values.reshape(-1)\n",
    "\n",
    "        a_0 = df['A'].values * np.cos(rad).values.reshape(-1)\n",
    "        a_1 = np.sin(rad)\n",
    "\n",
    "        df['v_0'] = v_0\n",
    "        df['v_1'] = v_1\n",
    "        df['a_0'] = a_0\n",
    "        df['a_1'] = a_1\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def back_features(df):\n",
    "        carriers = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','NflIdRusher','X','Y','Orientation','Dir','YardLine']]\n",
    "        carriers['back_from_scrimmage'] = carriers['YardLine'] - carriers['X']\n",
    "        carriers['back_oriented_down_field'] = carriers['Orientation'].apply(lambda x: back_direction(x))\n",
    "        carriers['back_moving_down_field'] = carriers['Dir'].apply(lambda x: back_direction(x))\n",
    "        carriers = carriers.rename(columns={'X':'back_X',\n",
    "                                            'Y':'back_Y'})\n",
    "        carriers = carriers[['GameId','PlayId','NflIdRusher','back_X','back_Y','back_from_scrimmage','back_oriented_down_field','back_moving_down_field']]\n",
    "\n",
    "        return carriers\n",
    "\n",
    "    def features_relative_to_back(df, carriers):\n",
    "        player_distance = df[['GameId','PlayId','NflId','X','Y']]\n",
    "        player_distance = pd.merge(player_distance, carriers, on=['GameId','PlayId'], how='inner')\n",
    "        player_distance = player_distance[player_distance['NflId'] != player_distance['NflIdRusher']]\n",
    "        player_distance['dist_to_back'] = player_distance[['X','Y','back_X','back_Y']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n",
    "\n",
    "        player_distance = player_distance.groupby(['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field'])\\\n",
    "                                         .agg({'dist_to_back':['min','max','mean','std']})\\\n",
    "                                         .reset_index()\n",
    "        player_distance.columns = ['GameId','PlayId','back_from_scrimmage','back_oriented_down_field','back_moving_down_field',\n",
    "                                   'min_dist','max_dist','mean_dist','std_dist']\n",
    "\n",
    "        return player_distance\n",
    "\n",
    "    def defense_features(df):\n",
    "        rusher = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','Team','X','Y']]\n",
    "        rusher.columns = ['GameId','PlayId','RusherTeam','RusherX','RusherY']\n",
    "\n",
    "        defense = pd.merge(df,rusher,on=['GameId','PlayId'],how='inner')\n",
    "        defense = defense[defense['Team'] != defense['RusherTeam']][['GameId','PlayId','X','Y','RusherX','RusherY']]\n",
    "        defense['def_dist_to_back'] = defense[['X','Y','RusherX','RusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n",
    "\n",
    "        defense = defense.groupby(['GameId','PlayId'])\\\n",
    "                         .agg({'def_dist_to_back':['min','max','mean','std']})\\\n",
    "                         .reset_index()\n",
    "        defense.columns = ['GameId','PlayId','def_min_dist','def_max_dist','def_mean_dist','def_std_dist']\n",
    "\n",
    "\n",
    "        return defense\n",
    "\n",
    "    def static_features(df):\n",
    "        static_features = df[df['NflId'] == df['NflIdRusher']][['GameId','PlayId','X','Y','S','A','Dis','Orientation','Dir','Quarter', 'Down',\n",
    "                                                            'YardLine','Distance','DefendersInTheBox']].drop_duplicates()\n",
    "        static_features['DefendersInTheBox'] = static_features['DefendersInTheBox'].fillna(np.mean(static_features['DefendersInTheBox']))\n",
    "\n",
    "        return static_features\n",
    "    \n",
    "    def other_features(df):\n",
    "        otf = df[['GameId','PlayId','NflId','X','Y', 'Team', 'A', 'PossessionTeam', 'S', 'NflIdRusher', \n",
    "                  'OffensePersonnel', 'DefensePersonnel', 'Dir', 'Dis', 'PlayerWeight', 'PlayerHeight',\n",
    "                 'GameWeather', 'TimeHandoff', 'TimeSnap', 'PlayerBirthDate', 'StadiumType']]\n",
    "        \n",
    "        otf['IsRusher'] = (otf['NflId'] == otf['NflIdRusher'])\n",
    "        otf['OffenseTeam'] = np.repeat(otf.loc[otf['IsRusher']==True, 'Team'].values, 22)\n",
    "        \n",
    "\n",
    "        \n",
    "        otf['IsOnOffense'] = (otf['Team'] == otf['OffenseTeam'])\n",
    "        otf['IsOnOffense'] = otf['IsOnOffense'].apply(int)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        average_offense_x = np.mean(otf.loc[otf['IsOnOffense'] == 1, 'X'].values.reshape((-1, 11)), axis=1)\n",
    "        try:\n",
    "            otf['OffenseCentroid_X'] = np.repeat(average_offense_x, 22)\n",
    "        except:\n",
    "            print(average_offense_x)\n",
    "            print(otf.shape)\n",
    "            print(otf.head())\n",
    "\n",
    "        average_defense_x = np.mean(otf.loc[otf['IsOnOffense'] == 0, 'X'].values.reshape((-1, 11)), axis=1)\n",
    "        otf['DefenseCentroid_X'] = np.repeat(average_defense_x, 22)\\\n",
    "\n",
    "        average_offense_y = np.mean(otf.loc[otf['IsOnOffense'] == 1, 'Y'].values.reshape((-1, 11)), axis=1)\n",
    "        otf['OffenseCentroid_Y'] = np.repeat(average_offense_x, 22)\n",
    "\n",
    "        average_defense_x = np.mean(otf.loc[otf['IsOnOffense'] == 0, 'Y'].values.reshape((-1, 11)), axis=1)\n",
    "        otf['DefenseCentroid_Y'] = np.repeat(average_defense_x, 22)\n",
    "\n",
    "      \n",
    "\n",
    "        def find_rusher_distance_to_ocent(row):\n",
    "\n",
    "            return np.sqrt((row['X'] - row['OffenseCentroid_X'])**2 + (row['Y'] - row['OffenseCentroid_Y'])**2)\n",
    "\n",
    "        def find_rusher_distance_to_dcent(row):\n",
    "            return np.sqrt((row['X'] - row['DefenseCentroid_X'])**2 + (row['Y'] - row['DefenseCentroid_Y'])**2)\n",
    "\n",
    "        rusher_centroids = otf.loc[otf['IsRusher'] == True, ['X', 'Y', 'OffenseCentroid_X','OffenseCentroid_Y','DefenseCentroid_X','DefenseCentroid_Y' ]]\n",
    "        otf['runner_distance_to_ocent'] = np.repeat(rusher_centroids.apply(find_rusher_distance_to_ocent, axis=1).values, 22)\n",
    "        otf['runner_distance_to_dcent'] = np.repeat(rusher_centroids.apply(find_rusher_distance_to_dcent, axis=1).values, 22)\n",
    "       \n",
    "        otf['time_to_tackle'] = 0\n",
    "        \n",
    "        otf['S'] = otf['S'].replace(to_replace=0, value=otf['S'].mean())\n",
    "        \n",
    "        runner_speeds = otf.loc[otf['IsRusher'] == True,'S']\n",
    "        otf['RunnerSpeed'] =  np.repeat(np.array(runner_speeds),22)\n",
    "\n",
    "        otf['RunnerSpeed'] = otf['RunnerSpeed'].replace(to_replace=0, value=otf['RunnerSpeed'].mean())\n",
    "        \n",
    "        otf['Rusher_X'] =  np.repeat(rusher_centroids['X'].values,22)\n",
    "        otf['Rusher_Y'] =  np.repeat(rusher_centroids['Y'].values,22)\n",
    "\n",
    "        \n",
    "        def find_distance_from_runner(row):\n",
    "            return np.sqrt((row['X'] - row['Rusher_X'])**2 + (row['Y'] - row['Rusher_Y'])**2)\n",
    "\n",
    "        otf['distance_from_runner'] = otf.apply(find_distance_from_runner, axis=1)\n",
    "\n",
    "        otf.loc[otf['IsOnOffense']==0, 'time_to_tackle'] = otf[otf['IsOnOffense']==0].apply(lambda row: row['distance_from_runner']/(row['S'] + row['RunnerSpeed']), axis=1)\n",
    "        otf['min_time_to_tackle'] = np.repeat(np.min(otf.loc[otf['IsOnOffense']==0, 'time_to_tackle'].values.reshape(-1, 11), axis=1), 22)\n",
    "        otf['average_time_to_tackle'] = np.repeat(np.mean(otf.loc[otf['IsOnOffense']==0, 'time_to_tackle'].values.reshape(-1, 11), axis=1), 22)\n",
    "        \n",
    "        defense_x_indexes = np.where(otf['IsOnOffense'] == 0)[0][::11]\n",
    "        \n",
    "        closest_defendor_indexes = np.argmin(otf.loc[otf['IsOnOffense'] == 0,'distance_from_runner'].values.reshape(-1,11), axis=1) + defense_x_indexes\n",
    "        otf['IsClosestDefendor'] = 0\n",
    "        otf.loc[closest_defendor_indexes, 'IsClosestDefendor'] = 1\n",
    "\n",
    "        runner_speeds = otf.loc[otf['IsRusher'] == True,'S']\n",
    "        firstdefensor_speeds = otf.loc[otf['IsClosestDefendor'] == 1,'S']\n",
    "        firstdefensor_speeds.replace([np.nan, 0], 1)\n",
    "        otf['runner_vs_1stdefensor_speed'] = np.repeat(runner_speeds.values / firstdefensor_speeds.values ,22)\n",
    "        otf['runner_vs_1stdefensor_speed'].fillna(np.mean(otf['runner_vs_1stdefensor_speed'].values))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        otf['off_average_acc'] = np.repeat(np.mean(otf.loc[otf['IsOnOffense']==1, 'A'].values.reshape(-1, 11), axis=1), 22)\n",
    "        otf['off_std_acc'] = np.repeat(np.std(otf.loc[otf['IsOnOffense']==1, 'A'].values.reshape(-1, 11), axis=1), 22)\n",
    "        otf['def_average_acc'] = np.repeat(np.mean(otf.loc[otf['IsOnOffense']==0, 'A'].values.reshape(-1, 11), axis=1), 22)\n",
    "        otf['def_std_acc'] = np.repeat(np.std(otf.loc[otf['IsOnOffense']==0, 'A'].values.reshape(-1, 11), axis=1), 22)\n",
    "        \n",
    "        otf['off_average_speed'] = np.repeat(np.mean(otf.loc[otf['IsOnOffense']==1, 'S'].values.reshape(-1, 11), axis=1), 22)\n",
    "        otf['def_average_speed'] = np.repeat(np.mean(otf.loc[otf['IsOnOffense']==0, 'S'].values.reshape(-1, 11), axis=1), 22)\n",
    "        \n",
    "        runner_speeds = otf.loc[otf['IsRusher'] == True,'A']\n",
    "        otf['1stdefensor_acc'] = np.repeat(otf.loc[otf['IsClosestDefendor'] == True,'A'].values, 22)\n",
    "        firstdefensor_speeds = otf.loc[otf['IsClosestDefendor'] == 1,'A']\n",
    "        firstdefensor_speeds.replace([np.nan, 0], 1, inplace=True)\n",
    "        otf['runner_vs_1stdefensor_acc'] = np.repeat(runner_speeds.values / firstdefensor_speeds.values ,22)\n",
    "        otf['runner_vs_1stdefensor_acc'].replace([np.nan, np.inf, -np.inf], 999, inplace=True)\n",
    "     \n",
    "     \n",
    "        ###\n",
    "        \n",
    "        otf['1stdefensor_dir'] = np.repeat(otf.loc[otf['IsClosestDefendor'] == True,'Dir'].values, 22)\n",
    "        \n",
    "        otf['fe1'] = pd.Series(np.sqrt(np.absolute(np.square(otf.X.values) - np.square(otf.Y.values))))\n",
    "        otf['fe5'] = np.square(otf['S'].values) + 2 * otf['A'].values * otf['Dis'].values  # N\n",
    "        otf['fe7'] = np.arccos(np.clip(otf['X'].values / otf['Y'].values, -1, 1))  # N\n",
    "        otf['fe8'] = otf['S'].values / np.clip(otf['fe1'].values, 0.6, None)\n",
    "        radian_angle = (90 - otf['Dir']) * np.pi / 180.0\n",
    "        otf['fe10'] = np.abs(otf['S'] * np.cos(radian_angle))\n",
    "        otf['fe11'] = np.abs(otf['S'] * np.sin(radian_angle))\n",
    "        otf['fe12'] = otf['S']*otf['PlayerWeight']\n",
    "\n",
    "        features_to_take = ['GameId','PlayId', \n",
    "                            'runner_distance_to_ocent', 'runner_distance_to_dcent', 'min_time_to_tackle', 'average_time_to_tackle',\n",
    "                             'off_average_acc', 'def_average_acc', 'off_average_speed', 'def_average_speed',\n",
    "                              'runner_vs_1stdefensor_speed' ,\n",
    "                            'IsRusher'\n",
    "                            ] + ['fe1', 'fe5', 'fe7', 'fe8' , 'fe10', 'fe11','fe12']\n",
    "        \n",
    "    \n",
    "\n",
    "        other_features = otf[features_to_take]\n",
    "\n",
    "        return other_features\n",
    "    \n",
    "\n",
    "\n",
    "    def combine_features(relative_to_back, defense, static, speed_frame, other_feats, deploy=deploy):\n",
    "        df = pd.merge(relative_to_back,defense,on=['GameId','PlayId'],how='inner')\n",
    "        df = pd.merge(df,static,on=['GameId','PlayId'],how='inner')\n",
    "        df = pd.merge(df, speed_frame, on=['GameId','PlayId'],how='inner')\n",
    "        df = pd.merge(df, other_feats, on=['GameId','PlayId'],how='inner')\n",
    "        #f = pd.merge(df, personnel_feats, on=['GameId','PlayId'],how='inner')\n",
    "        if not deploy:\n",
    "            df = pd.merge(df, outcomes, on=['GameId','PlayId'], how='inner')\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    df = projection_features(df)\n",
    "    speed_frame= df[['GameId','PlayId',\n",
    "   \n",
    "                     'a_0','a_1','v_0','v_1', 'Temperature', 'Humidity']]\n",
    "    \n",
    "    yardline = update_yardline(df)\n",
    "    df = update_orientation(df, yardline)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "                \n",
    "    other_feats = other_features(df)\n",
    "    back_feats = back_features(df)\n",
    "    rel_back = features_relative_to_back(df, back_feats)\n",
    "    def_feats = defense_features(df)\n",
    "    static_feats = static_features(df)\n",
    "\n",
    "    \n",
    "    basetable = combine_features(rel_back, def_feats, static_feats,  speed_frame, other_feats,  deploy=deploy)\n",
    "    basetable[['a_0','a_1','v_0','v_1']] = basetable[['a_0','a_1','v_0','v_1']].fillna(0)\n",
    "    basetable['Temperature'] = basetable['Temperature'].fillna(60.436442)\n",
    "    basetable['Humidity'] = basetable['Humidity'].fillna(59.0)\n",
    "    basetable = basetable[basetable['IsRusher'] ==True]\n",
    "    basetable.drop(['IsRusher'], axis=1, inplace=True)\n",
    "\n",
    " \n",
    "\n",
    "    return basetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_logs(basetable):\n",
    "   \n",
    "    log_basetable = pd.DataFrame(np.log(basetable.values), columns=['LOG_' + str(a) for a in basetable.columns])\n",
    "    log_basetable.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    #log_basetable.dropna(axis=1, inplace=True)\n",
    "    log_basetable.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    basetable = pd.concat([pd.DataFrame(basetable.values, columns=basetable.columns), log_basetable], axis=1)\n",
    "    \n",
    "    return basetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:181: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:182: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:207: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:219: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:220: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:222: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:224: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:227: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:229: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:232: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:241: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:242: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:247: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:253: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:264: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:265: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:268: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:271: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py:6786: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:277: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:279: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:280: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:281: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 17s, sys: 25.2 s, total: 3min 42s\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "source": [
    "%time train_basetable1 = create_features_orig(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basetable1.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_basetable1.columns:\n",
    "    if train_basetable1[col].dtype == 'object':\n",
    "        train_basetable1.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.07 s, sys: 496 ms, total: 1.56 s\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%time train_basetable = add_logs(train_basetable1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_basetable.copy()\n",
    "yards = X.Yards\n",
    "\n",
    "y = np.zeros((yards.shape[0], 199))\n",
    "for idx, target in enumerate(list(yards)):\n",
    "\n",
    "    y[int(idx)][99 + int(target)] = 1\n",
    "\n",
    "X.drop(['Yards', 'GameId', 'PlayId', 'LOG_GameId', 'LOG_PlayId','LOG_Yards'], axis=1, inplace=True)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric(Callback):\n",
    "    def __init__(self, model, callbacks, data):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.callbacks = callbacks\n",
    "        self.data = data\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_begin(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_end(logs)\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "#         X_train, y_train = self.data[0][0], self.data[0][1]\n",
    "#         y_pred = self.model.predict(X_train)\n",
    "#         y_true = np.clip(np.cumsum(y_train, axis=1), 0, 1)\n",
    "#         y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "#         tr_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_train.shape[0])\n",
    "#         tr_s = np.round(tr_s, 6)\n",
    "        logs['tr_CRPS'] = 0\n",
    "\n",
    "        a_val,  X_valid,  y_valid = self.data[1][0][0], self.data[1][0][1], self.data[1][1]\n",
    "        \n",
    "        y_pred = self.model.predict([a_val, X_valid])\n",
    "        \n",
    "        def to_dist(val):\n",
    "            y_pred = np.zeros((val.shape[0], 199))\n",
    "            for idx, target in enumerate(list(val)):\n",
    "                y_pred[int(idx)][99 + int(target):] = 1\n",
    "            return y_pred\n",
    "    \n",
    "        y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n",
    "        y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "        val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid.shape[0])\n",
    "        val_s = np.round(val_s, 6)\n",
    "        logs['val_CRPS'] = val_s\n",
    "        print('tr CRPS', 'Grr', 'val CRPS', val_s)\n",
    "\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Reshape, Concatenate, Flatten, GaussianNoise\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "from keras.backend import sigmoid, tanh, softplus\n",
    "def swish(x, beta = 1):\n",
    "     return (x * sigmoid(beta * x))\n",
    "    \n",
    "def mish(x):\n",
    "    return(x * tanh(softplus(x)))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish), 'mish':Activation(mish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def get_model0():\n",
    "    \n",
    "\n",
    "    \n",
    "    embeddings = []\n",
    "    inputs = []\n",
    "    #OffenseFormation\n",
    "    input0 = Input(shape=(1,))\n",
    "    embedding0 = Embedding(7, 3, input_length=1)(input0)\n",
    "    embedding0 = Reshape(target_shape=(3,))(embedding0)\n",
    "  \n",
    "    inputs.append(input0)\n",
    "    embeddings.append(embedding0)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    input4 = Input(shape=(X.shape[1],))\n",
    "    inputs.append(input4)\n",
    "    \n",
    "    x = Concatenate()(inputs)\n",
    "\n",
    "\n",
    "    x= Dense(512, activation='mish')(x)\n",
    "    #model.add(ReLU())\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dropout(0.68)(x)\n",
    "    \n",
    "    x= Dense(256, activation='mish')(x)\n",
    "    #model.add(ReLU())\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dropout(0.68)(x)\n",
    "\n",
    "    x= Dense(199, activation='mish')(x)\n",
    "    #model.add(ReLU())\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dropout(0.68)(x)\n",
    "\n",
    "\n",
    "    \n",
    "    output = Dense(199, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, output)\n",
    "  \n",
    "    model.compile(optimizer=Adam(lr=0.002, clipvalue=0.5), loss='categorical_crossentropy', metrics=[])\n",
    "    \n",
    "    \n",
    "   \n",
    "    return model\n",
    "\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[     0      1      2 ... 509759 509760 509761]\n",
      "[  7172   7173   7174 ... 496713 496714 496715]\n",
      "Train on 463628 samples, validate on 46134 samples\n",
      "Epoch 1/250\n",
      "463628/463628 [==============================] - 22s 47us/step - loss: 3.6236 - val_loss: 2.7621\n",
      "tr CRPS Grr val CRPS 0.013282\n",
      "Epoch 2/250\n",
      "463628/463628 [==============================] - 21s 45us/step - loss: 2.7807 - val_loss: 2.7238\n",
      "tr CRPS Grr val CRPS 0.013001\n",
      "Epoch 3/250\n",
      "463628/463628 [==============================] - 21s 45us/step - loss: 2.7476 - val_loss: 2.7103\n",
      "tr CRPS Grr val CRPS 0.012884\n",
      "Epoch 4/250\n",
      "463628/463628 [==============================] - 21s 45us/step - loss: 2.7302 - val_loss: 2.7018\n",
      "tr CRPS Grr val CRPS 0.012802\n",
      "Epoch 5/250\n",
      "463628/463628 [==============================] - 21s 45us/step - loss: 2.7145 - val_loss: 2.6912\n",
      "tr CRPS Grr val CRPS 0.012754\n",
      "Epoch 6/250\n",
      "463628/463628 [==============================] - 21s 45us/step - loss: 2.7006 - val_loss: 2.6917\n",
      "tr CRPS Grr val CRPS 0.012712\n",
      "Epoch 7/250\n",
      "463628/463628 [==============================] - 21s 46us/step - loss: 2.6878 - val_loss: 2.6855\n",
      "tr CRPS Grr val CRPS 0.012679\n",
      "Epoch 8/250\n",
      "463628/463628 [==============================] - 21s 46us/step - loss: 2.6774 - val_loss: 2.6856\n",
      "tr CRPS Grr val CRPS 0.012689\n",
      "Epoch 9/250\n",
      "463628/463628 [==============================] - 21s 45us/step - loss: 2.6669 - val_loss: 2.6916\n",
      "tr CRPS Grr val CRPS 0.012683\n",
      "Epoch 10/250\n",
      "463628/463628 [==============================] - 21s 46us/step - loss: 2.6586 - val_loss: 2.6887\n",
      "tr CRPS Grr val CRPS 0.012655\n",
      "Epoch 11/250\n",
      "463628/463628 [==============================] - 21s 46us/step - loss: 2.6505 - val_loss: 2.6824\n",
      "tr CRPS Grr val CRPS 0.012632\n",
      "Epoch 12/250\n",
      "463628/463628 [==============================] - 21s 46us/step - loss: 2.6406 - val_loss: 2.6853\n",
      "tr CRPS Grr val CRPS 0.012659\n",
      "Epoch 13/250\n",
      "463628/463628 [==============================] - 21s 46us/step - loss: 2.6325 - val_loss: 2.6802\n",
      "tr CRPS Grr val CRPS 0.012653\n",
      "Epoch 14/250\n",
      "463628/463628 [==============================] - 21s 46us/step - loss: 2.6242 - val_loss: 2.6876\n",
      "tr CRPS Grr val CRPS 0.012671\n",
      "Epoch 15/250\n",
      "463628/463628 [==============================] - 21s 45us/step - loss: 2.6170 - val_loss: 2.6909\n",
      "tr CRPS Grr val CRPS 0.012669\n",
      "Epoch 16/250\n",
      "463628/463628 [==============================] - 21s 45us/step - loss: 2.6101 - val_loss: 2.6910\n",
      "tr CRPS Grr val CRPS 0.012693\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00016: early stopping\n",
      "1\n",
      "[     0      1      2 ... 509759 509760 509761]\n",
      "[  2112   2113   2114 ... 485317 485318 485319]\n",
      "Train on 463760 samples, validate on 46002 samples\n",
      "Epoch 1/250\n",
      "463760/463760 [==============================] - 22s 47us/step - loss: 3.6094 - val_loss: 2.7979\n",
      "tr CRPS Grr val CRPS 0.013333\n",
      "Epoch 2/250\n",
      "463760/463760 [==============================] - 21s 46us/step - loss: 2.7770 - val_loss: 2.7509\n",
      "tr CRPS Grr val CRPS 0.013021\n",
      "Epoch 3/250\n",
      "463760/463760 [==============================] - 21s 44us/step - loss: 2.7456 - val_loss: 2.7367\n",
      "tr CRPS Grr val CRPS 0.012932\n",
      "Epoch 4/250\n",
      "463760/463760 [==============================] - 21s 45us/step - loss: 2.7278 - val_loss: 2.7221\n",
      "tr CRPS Grr val CRPS 0.012845\n",
      "Epoch 5/250\n",
      "463760/463760 [==============================] - 21s 44us/step - loss: 2.7114 - val_loss: 2.7158\n",
      "tr CRPS Grr val CRPS 0.012767\n",
      "Epoch 6/250\n",
      "463760/463760 [==============================] - 20s 44us/step - loss: 2.6979 - val_loss: 2.7052\n",
      "tr CRPS Grr val CRPS 0.012722\n",
      "Epoch 7/250\n",
      "463760/463760 [==============================] - 21s 44us/step - loss: 2.6865 - val_loss: 2.7017\n",
      "tr CRPS Grr val CRPS 0.012686\n",
      "Epoch 8/250\n",
      "463760/463760 [==============================] - 21s 44us/step - loss: 2.6757 - val_loss: 2.6965\n",
      "tr CRPS Grr val CRPS 0.012659\n",
      "Epoch 9/250\n",
      "463760/463760 [==============================] - 21s 45us/step - loss: 2.6659 - val_loss: 2.6932\n",
      "tr CRPS Grr val CRPS 0.012639\n",
      "Epoch 10/250\n",
      "463760/463760 [==============================] - 21s 44us/step - loss: 2.6574 - val_loss: 2.6905\n",
      "tr CRPS Grr val CRPS 0.012609\n",
      "Epoch 11/250\n",
      "463760/463760 [==============================] - 20s 44us/step - loss: 2.6477 - val_loss: 2.6921\n",
      "tr CRPS Grr val CRPS 0.012612\n",
      "Epoch 12/250\n",
      "463760/463760 [==============================] - 20s 44us/step - loss: 2.6392 - val_loss: 2.6887\n",
      "tr CRPS Grr val CRPS 0.012574\n",
      "Epoch 13/250\n",
      "463760/463760 [==============================] - 20s 44us/step - loss: 2.6311 - val_loss: 2.6947\n",
      "tr CRPS Grr val CRPS 0.012592\n",
      "Epoch 14/250\n",
      "463760/463760 [==============================] - 21s 45us/step - loss: 2.6235 - val_loss: 2.6929\n",
      "tr CRPS Grr val CRPS 0.012563\n",
      "Epoch 15/250\n",
      "463760/463760 [==============================] - 20s 44us/step - loss: 2.6149 - val_loss: 2.6984\n",
      "tr CRPS Grr val CRPS 0.012566\n",
      "Epoch 16/250\n",
      "463760/463760 [==============================] - 21s 45us/step - loss: 2.6086 - val_loss: 2.6953\n",
      "tr CRPS Grr val CRPS 0.012564\n",
      "Epoch 17/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.7304 - val_loss: 2.6995\n",
      "tr CRPS Grr val CRPS 0.012834\n",
      "Epoch 5/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.7147 - val_loss: 2.6901\n",
      "tr CRPS Grr val CRPS 0.012783\n",
      "Epoch 6/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.7013 - val_loss: 2.6822\n",
      "tr CRPS Grr val CRPS 0.012724\n",
      "Epoch 7/250\n",
      "463122/463122 [==============================] - 21s 44us/step - loss: 2.6889 - val_loss: 2.6738\n",
      "tr CRPS Grr val CRPS 0.012687\n",
      "Epoch 8/250\n",
      "463122/463122 [==============================] - 21s 46us/step - loss: 2.6782 - val_loss: 2.6741\n",
      "tr CRPS Grr val CRPS 0.012678\n",
      "Epoch 9/250\n",
      "463122/463122 [==============================] - 21s 46us/step - loss: 2.6669 - val_loss: 2.6753\n",
      "tr CRPS Grr val CRPS 0.012663\n",
      "Epoch 10/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6583 - val_loss: 2.6751\n",
      "tr CRPS Grr val CRPS 0.012656\n",
      "Epoch 11/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6498 - val_loss: 2.6778\n",
      "tr CRPS Grr val CRPS 0.012652\n",
      "Epoch 12/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6396 - val_loss: 2.6719\n",
      "tr CRPS Grr val CRPS 0.012667\n",
      "Epoch 13/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6323 - val_loss: 2.6749\n",
      "tr CRPS Grr val CRPS 0.012648\n",
      "Epoch 14/250\n",
      "463122/463122 [==============================] - 21s 44us/step - loss: 2.6221 - val_loss: 2.6771\n",
      "tr CRPS Grr val CRPS 0.012668\n",
      "Epoch 15/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6154 - val_loss: 2.6778\n",
      "tr CRPS Grr val CRPS 0.012653\n",
      "Epoch 16/250\n",
      "463122/463122 [==============================] - 21s 44us/step - loss: 2.6086 - val_loss: 2.6788\n",
      "tr CRPS Grr val CRPS 0.012662\n",
      "Epoch 17/250\n",
      "463122/463122 [==============================] - 21s 44us/step - loss: 2.6001 - val_loss: 2.6857\n",
      "tr CRPS Grr val CRPS 0.012705\n",
      "Epoch 18/250\n",
      "463122/463122 [==============================] - 21s 44us/step - loss: 2.5952 - val_loss: 2.6869\n",
      "tr CRPS Grr val CRPS 0.012681\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00018: early stopping\n",
      "3\n",
      "[     0      1      2 ... 509759 509760 509761]\n",
      "[  1144   1145   1146 ... 502697 502698 502699]\n",
      "Train on 463122 samples, validate on 46640 samples\n",
      "Epoch 1/250\n",
      "463122/463122 [==============================] - 22s 47us/step - loss: 3.6334 - val_loss: 2.7616\n",
      "tr CRPS Grr val CRPS 0.012705\n",
      "Epoch 2/250\n",
      "463122/463122 [==============================] - 21s 46us/step - loss: 2.7832 - val_loss: 2.7136\n",
      "tr CRPS Grr val CRPS 0.012378\n",
      "Epoch 3/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.7508 - val_loss: 2.7035\n",
      "tr CRPS Grr val CRPS 0.012272\n",
      "Epoch 4/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.7319 - val_loss: 2.6939\n",
      "tr CRPS Grr val CRPS 0.012208\n",
      "Epoch 5/250\n",
      "463122/463122 [==============================] - 22s 47us/step - loss: 2.7172 - val_loss: 2.6860\n",
      "tr CRPS Grr val CRPS 0.012151\n",
      "Epoch 6/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.7035 - val_loss: 2.6823\n",
      "tr CRPS Grr val CRPS 0.01209\n",
      "Epoch 7/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6919 - val_loss: 2.6730\n",
      "tr CRPS Grr val CRPS 0.012061\n",
      "Epoch 8/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6805 - val_loss: 2.6696\n",
      "tr CRPS Grr val CRPS 0.012033\n",
      "Epoch 9/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6701 - val_loss: 2.6662\n",
      "tr CRPS Grr val CRPS 0.012005\n",
      "Epoch 10/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6618 - val_loss: 2.6664\n",
      "tr CRPS Grr val CRPS 0.012024\n",
      "Epoch 11/250\n",
      "463122/463122 [==============================] - 21s 46us/step - loss: 2.6527 - val_loss: 2.6657\n",
      "tr CRPS Grr val CRPS 0.012003\n",
      "Epoch 12/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6437 - val_loss: 2.6645\n",
      "tr CRPS Grr val CRPS 0.011971\n",
      "Epoch 13/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6356 - val_loss: 2.6721\n",
      "tr CRPS Grr val CRPS 0.012\n",
      "Epoch 14/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6281 - val_loss: 2.6728\n",
      "tr CRPS Grr val CRPS 0.012017\n",
      "Epoch 15/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6203 - val_loss: 2.6720\n",
      "tr CRPS Grr val CRPS 0.012016\n",
      "Epoch 16/250\n",
      "463122/463122 [==============================] - 21s 44us/step - loss: 2.6123 - val_loss: 2.6776\n",
      "tr CRPS Grr val CRPS 0.012061\n",
      "Epoch 17/250\n",
      "463122/463122 [==============================] - 21s 45us/step - loss: 2.6069 - val_loss: 2.6707\n",
      "tr CRPS Grr val CRPS 0.012013\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00017: early stopping\n",
      "4\n",
      "[     0      1      2 ... 509759 509760 509761]\n",
      "[ 11792  11793  11794 ... 491653 491654 491655]\n",
      "Train on 463144 samples, validate on 46618 samples\n",
      "Epoch 1/250\n",
      "463144/463144 [==============================] - 22s 48us/step - loss: 3.6244 - val_loss: 2.7338\n",
      "tr CRPS Grr val CRPS 0.013134\n",
      "Epoch 2/250\n",
      "463144/463144 [==============================] - 21s 44us/step - loss: 2.7827 - val_loss: 2.7044\n",
      "tr CRPS Grr val CRPS 0.012916\n",
      "Epoch 3/250\n",
      "463144/463144 [==============================] - 21s 45us/step - loss: 2.7508 - val_loss: 2.6930\n",
      "tr CRPS Grr val CRPS 0.012836\n",
      "Epoch 4/250\n",
      "463144/463144 [==============================] - 21s 46us/step - loss: 2.7318 - val_loss: 2.6870\n",
      "tr CRPS Grr val CRPS 0.012799\n",
      "Epoch 5/250\n",
      "463144/463144 [==============================] - 21s 45us/step - loss: 2.7174 - val_loss: 2.6775\n",
      "tr CRPS Grr val CRPS 0.012736\n",
      "Epoch 6/250\n",
      "463782/463782 [==============================] - 20s 44us/step - loss: 2.6583 - val_loss: 2.6769\n",
      "tr CRPS Grr val CRPS 0.012212\n",
      "Epoch 11/250\n",
      "463782/463782 [==============================] - 20s 44us/step - loss: 2.6492 - val_loss: 2.6807\n",
      "tr CRPS Grr val CRPS 0.012226\n",
      "Epoch 12/250\n",
      "463782/463782 [==============================] - 20s 44us/step - loss: 2.6410 - val_loss: 2.6851\n",
      "tr CRPS Grr val CRPS 0.012254\n",
      "Epoch 13/250\n",
      "463782/463782 [==============================] - 20s 44us/step - loss: 2.6329 - val_loss: 2.6889\n",
      "tr CRPS Grr val CRPS 0.012245\n",
      "Epoch 14/250\n",
      "463782/463782 [==============================] - 20s 44us/step - loss: 2.6243 - val_loss: 2.6827\n",
      "tr CRPS Grr val CRPS 0.012238\n",
      "Epoch 15/250\n",
      "463782/463782 [==============================] - 21s 45us/step - loss: 2.6154 - val_loss: 2.6880\n",
      "tr CRPS Grr val CRPS 0.012243\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00015: early stopping\n",
      "10\n",
      "[     0      1      2 ... 509759 509760 509761]\n",
      "[  8162   8163   8164 ... 507185 507186 507187]\n",
      "Train on 463782 samples, validate on 45980 samples\n",
      "Epoch 1/250\n",
      "463782/463782 [==============================] - 21s 46us/step - loss: 3.6289 - val_loss: 2.7840\n",
      "tr CRPS Grr val CRPS 0.013114\n",
      "Epoch 2/250\n",
      "463782/463782 [==============================] - 20s 44us/step - loss: 2.7785 - val_loss: 2.7416\n",
      "tr CRPS Grr val CRPS 0.012869\n",
      "Epoch 3/250\n",
      "463782/463782 [==============================] - 20s 44us/step - loss: 2.7483 - val_loss: 2.7310\n",
      "tr CRPS Grr val CRPS 0.012787\n",
      "Epoch 4/250\n",
      "463782/463782 [==============================] - 21s 44us/step - loss: 2.7298 - val_loss: 2.7177\n",
      "tr CRPS Grr val CRPS 0.012722\n",
      "Epoch 5/250\n",
      "463782/463782 [==============================] - 21s 44us/step - loss: 2.7131 - val_loss: 2.7114\n",
      "tr CRPS Grr val CRPS 0.012663\n",
      "Epoch 6/250\n",
      "463782/463782 [==============================] - 22s 46us/step - loss: 2.6994 - val_loss: 2.7098\n",
      "tr CRPS Grr val CRPS 0.012628\n",
      "Epoch 7/250\n",
      "463782/463782 [==============================] - 21s 44us/step - loss: 2.6875 - val_loss: 2.7059\n",
      "tr CRPS Grr val CRPS 0.012601\n",
      "Epoch 8/250\n",
      "391168/463782 [========================>.....] - ETA: 3s - loss: 2.6765"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "k = 0\n",
    "\n",
    "rkf = GroupKFold(11)\n",
    "for fold_id, (tr_idx, vl_idx) in enumerate(rkf.split(train_basetable['GameId'],train_basetable['GameId'],train_basetable['GameId'])):\n",
    "    print(fold_id)\n",
    "    print(tr_idx)\n",
    "    print(vl_idx)\n",
    "    x_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "    x_vl, y_vl = X[vl_idx], y[vl_idx]    \n",
    "    \n",
    "    a = train['OffenseFormation'].values[tr_idx]\n",
    "\n",
    "    a_val = train['OffenseFormation'].values[vl_idx]\n",
    "\n",
    "\n",
    "    model= eval(\"get_model\" + str((k%1)) + \"()\")\n",
    "    es = EarlyStopping(monitor='val_CRPS', \n",
    "                   mode='min',\n",
    "                   restore_best_weights=True, \n",
    "                   verbose=1, \n",
    "                   patience=5)\n",
    "    es.set_model(model)\n",
    "    metric = Metric(model, [es], [([a,x_tr], y_tr), ([ a_val,  x_vl], y_vl)])\n",
    "    \n",
    "\n",
    "\n",
    "    model.fit([a,x_tr], y_tr, callbacks=[metric], epochs=250, batch_size=2048, validation_data=[[a_val, x_vl], y_vl])\n",
    "    models.append(model)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_basetable1.copy()\n",
    "yards = X.Yards\n",
    "\n",
    "y = np.zeros((yards.shape[0], 199))\n",
    "for idx, target in enumerate(list(yards)):\n",
    "\n",
    "    y[int(idx)][99 + int(target)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['Yards', 'GameId', 'PlayId'], axis=1, inplace=True)\n",
    "scaler1 = StandardScaler()\n",
    "X = scaler1.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crps(y_true, y_pred):\n",
    "    y_true = np.clip(np.cumsum(y_true, axis=1), 0, 1)\n",
    "    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "    return ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * y_true.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "0.01251392004466005\n",
      "Fold : 1\n",
      "0.012557834040438047\n",
      "Fold : 2\n",
      "0.014282314572460963\n",
      "0.013118022885853021\n"
     ]
    }
   ],
   "source": [
    "treemodels = []\n",
    "kf = KFold(n_splits=3, random_state=42)\n",
    "score = []\n",
    "for i, (tdx, vdx) in enumerate(kf.split(X, y)):\n",
    "    print(f'Fold : {i}')\n",
    "    X_train, X_val, y_train, y_val = X[tdx], X[vdx], y[tdx], y[vdx]\n",
    "\n",
    "    model = RandomForestRegressor(bootstrap=False, max_features=0.2, min_samples_leaf=260, min_samples_split=7, n_estimators=70, n_jobs=-1, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    score_ = crps(y_val, model.predict(X_val))\n",
    "\n",
    "    print(score_)\n",
    "    score.append(score_)\n",
    "    treemodels.append(model)\n",
    "\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission file has been saved!  Once you `Commit` your Notebook and it finishes running, you can submit the file to the competition from the Notebook Viewer `Output` tab.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from kaggle.competitions import nflrush\n",
    "env = nflrush.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "\n",
    "    basetable1 = create_features_orig(test_df, deploy=True)\n",
    "    basetable1.fillna(0, inplace=True)\n",
    "    basetable = add_logs(basetable1)\n",
    "    basetable.drop(['GameId', 'PlayId', 'LOG_GameId', 'LOG_PlayId'], axis=1, inplace=True)\n",
    "\n",
    "    scaled_basetable = scaler.transform(basetable)\n",
    "    try:\n",
    "        a = test_df['OffenseFormation'] = test_df['OffenseFormation'].apply(lambda x: of_index[x])\n",
    "    except:\n",
    "        test_df['OffenseFormation'] = np.nan\n",
    "        test_df['OffenseFormation'].fillna(-1, inplace=True)\n",
    "        a = test_df['OffenseFormation']\n",
    "\n",
    "  \n",
    "    p = [m.predict([a,scaled_basetable]) for m in models]\n",
    "    \n",
    "    y_pred_nn = (np.array(p)).mean(0)\n",
    "    #np.expm1(np.log1p(np.array(p)).mean(0))\n",
    "    \n",
    "    basetable1.drop(['GameId', 'PlayId'], axis=1, inplace=True)\n",
    "    scaled_basetable1 = scaler1.transform(basetable1)\n",
    "    y_pred_tree = np.array([m.predict(scaled_basetable1) for m in treemodels]).mean(0)\n",
    "    \n",
    "   \n",
    "    y_pred = (.85*y_pred_nn+.15*y_pred_tree)\n",
    "    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
    "    y_pred[:,:90] = 0\n",
    "    #y_pred[:,122:] = 1\n",
    "    y_pred = y_pred.tolist()[0]\n",
    "\n",
    "    preds_df = pd.DataFrame(data=[y_pred], columns=sample_prediction_df.columns)\n",
    "    env.predict(preds_df)\n",
    "\n",
    "\n",
    "    \n",
    "env.write_submission_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
